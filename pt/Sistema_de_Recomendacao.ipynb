{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhkvW_EIkE-a"
      },
      "source": [
        "#Configurações do arquivo\n",
        "- ambiente rodado no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0dDdBFxkUjM",
        "outputId": "b1cf77f8-fc14-416a-d512-28523703991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise) (1.7.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp38-cp38-linux_x86_64.whl size=2626460 sha256=8009cb53d1733c2f3df7deaeda9a70262ccc16d76b81f0251a25d3d5ef6a76b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/db/86/2c18183a80ba05da35bf0fb7417aac5cddbd93bcb1b92fd3ea\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9FvaaJFTrS7",
        "outputId": "d9a63f12-270a-4483-8831-11eda59ac027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Sat Dec 10 14:52:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#google colab pro feature\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toDD9c5T5Pvk"
      },
      "source": [
        "## Geração low-level MPEG-7\n",
        "- O conceito é que enriquecer as características com variáveis ​​derivadas deve aumentar o desempenho do modelo. É por isso que ainda é uma boa ideia tentar isso no conjunto de dados de comércio eletrônico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70E9iy2MkBMz",
        "outputId": "9b4ced59-210d-42fa-b69e-356655bc7c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plMmWC_9NH70"
      },
      "source": [
        "# Variaveis de configuração e constrantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xSOdwNSns-i"
      },
      "outputs": [],
      "source": [
        "#parametros globais\n",
        "\n",
        "# FEATURE_NAMES = ['game_duration_sum_rank_1']\n",
        "FEATURE_NAMES = ['total_followers', 'team_id', 'qtd_emotes', 'Português', 'Brasil',\n",
        "       'Portugal', 'Competitivo', '100%', 'AMA', 'Multijogador', 'LGBTQIA+',\n",
        "       'Jogada casual', 'Drops habilitados', 'Conversação', 'PvP',\n",
        "       'Jogando com espectadores', 'Anime', 'Jogada de ranking',\n",
        "       'Atuação e encenação', 'PC', 'Jogadores iniciantes', 'Primeira jogada',\n",
        "       'Futebol', 'Aconchegante', 'Relaxada', 'Inglês', 'Único jogador',\n",
        "       'Modo: Battle Royale', 'Repetir', 'Vtuber', 'Cooperativo',\n",
        "       'Jogador X ambiente', 'Modo: Aventura', 'Depressão', 'Esports',\n",
        "       'Modo: Sobrevivência', 'Retrô', 'Jogo de guerra', 'Mapa: Mirage',\n",
        "       'Arte digital', 'Mulher', 'Desenho', 'ASMR auditivo',\n",
        "       'Para toda a família', 'Corrida', 'EUA', 'TDAH', 'Extensão aprimorada',\n",
        "       'Modo difícil', 'Inglaterra', 'Animada', 'Sem palpites',\n",
        "       'Espaço seguro', 'menor_trecho_game', 'maior_trecho_game',\n",
        "       'game_duration_sum_rank_1', 'game_duration_sum_rank_2',\n",
        "       'game_duration_sum_rank_3', 'game_duration_sum_rank_4',\n",
        "       'game_duration_sum_rank_5', 'game_duration_sum_rank_6',\n",
        "       'game_duration_sum_rank_7', 'game_duration_sum_rank_8',\n",
        "       'game_duration_sum_rank_9', 'game_duration_sum_rank_10',\n",
        "       'game_duration_count_rank_1', 'game_duration_count_rank_2',\n",
        "       'game_duration_count_rank_3', 'game_duration_count_rank_4',\n",
        "       'game_duration_count_rank_5', 'game_duration_count_rank_6',\n",
        "       'game_duration_count_rank_7', 'game_duration_count_rank_8',\n",
        "       'game_duration_count_rank_9', 'game_duration_count_rank_10',\n",
        "       'game_duration_mean_rank_1', 'game_duration_mean_rank_2',\n",
        "       'game_duration_mean_rank_3', 'game_duration_mean_rank_4',\n",
        "       'game_duration_mean_rank_5', 'game_duration_mean_rank_6',\n",
        "       'game_duration_mean_rank_7', 'game_duration_mean_rank_8',\n",
        "       'game_duration_mean_rank_9', 'game_duration_mean_rank_10',\n",
        "       'rank_jogos_recentes_1', 'rank_jogos_recentes_2',\n",
        "       'rank_jogos_recentes_3', 'rank_jogos_recentes_4',\n",
        "       'rank_jogos_recentes_5', 'rank_jogos_recentes_6',\n",
        "       'rank_jogos_recentes_7', 'rank_jogos_recentes_8',\n",
        "       'rank_jogos_recentes_9', 'rank_jogos_recentes_10']\n",
        "PATH = \"/content/drive/MyDrive/db_tcc\"\n",
        "# RATINGS_PATH = f'{PATH}/base_independentes/ratings_similarity.csv'\n",
        "RATINGS_PATH = f'{PATH}/base_independentes/ratings_sample_5kk.csv'\n",
        "CANAIS_PATH = f'{PATH}/all_metrics_filter.csv'\n",
        "TEST_SIZE=0.4\n",
        "USER_KEY_TEST = 59593070 #patripopes\n",
        "K_TOP= 10\n",
        "TOP_K = 10\n",
        "#verificar trainset de funçao fit\n",
        "#verificar thisRating % 100 pois essa é a intereçao do "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhiotCL5NRqy"
      },
      "source": [
        "# Códigos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWsi5y5PN1LZ"
      },
      "source": [
        "## Objeto canal Twitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTKNPZwCL6Ri"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class CanaisTwitch:\n",
        "\n",
        "    ratingsDataset = {}\n",
        "    name_to_id = {}\n",
        "    \n",
        "    def loadData(self): \n",
        "        #nome da função antiga é LoadMovieLensData \n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.id_to_name = {} #dict id_canal: nome_canal\n",
        "        # self.name_to_id = {} #opta-se não utilizar os nomes dos canais e só o ID é o suficiente #dict noma_canal_como_chave: id_canal\n",
        "\n",
        "        # reader = Reader(line_format='user item rating', sep=';', skip_lines=1) #broadcast_id, item_from_broadcast_id, rating [nao tem coluna timestamp]\n",
        "        # # ratingsDataset = Dataset.load_from_file(RATINGS_PATH, reader=reader)\n",
        "        reader = Reader(line_format='user item rating', sep=';', skip_lines=1)\n",
        "        ratingsDataset = Dataset.load_from_file(RATINGS_PATH, reader=reader)\n",
        "\n",
        "        canaisPath = pd.read_csv(CANAIS_PATH, sep=\";\")\n",
        "        for canal_id in canaisPath['broadcaster_id'].values.tolist():\n",
        "          self.name_to_id[canal_id]=f'{canal_id}'\n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getPopularityRanks(self): #ESSE USA\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(RATINGS_PATH, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile, delimiter=';')\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                movieID = int(row[1])\n",
        "                ratings[movieID] += 1\n",
        "        rank = 1\n",
        "        for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[movieID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "\n",
        "    def getFeatures(self):\n",
        "      features = defaultdict(list)\n",
        "      \n",
        "      dataset = pd.read_csv(CANAIS_PATH, sep=\";\")\n",
        "      for index, row in dataset.iterrows():\n",
        "          features[row['broadcaster_id']]=row[FEATURE_NAMES].values.tolist()\n",
        "\n",
        "      return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpdQgtDH9BEo"
      },
      "source": [
        "## Algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evWtajvinthT"
      },
      "outputs": [],
      "source": [
        "from surprise import AlgoBase\n",
        "from surprise import PredictionImpossible\n",
        "import math\n",
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "class ContentKNNAlgorithm(AlgoBase):\n",
        "\n",
        "    def __init__(self, k=40, sim_options={}):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        # Compute item similarity matrix based on content attributes\n",
        "\n",
        "        # Load up genre vectors for every movie\n",
        "        obj_dataset = CanaisTwitch()\n",
        "        features = obj_dataset.getFeatures()\n",
        "        # years = ml.getYears()\n",
        "        # mes = ml.getMiseEnScene()\n",
        "        \n",
        "        print(\"Computing content-based similarity matrix...\")\n",
        "            \n",
        "        # Compute genre distance for every movie combination as a 2x2 matrix\n",
        "        self.similarities = np.zeros((self.trainset.n_items, self.trainset.n_items))\n",
        "        \n",
        "        for thisRating in range(self.trainset.n_items):\n",
        "            if (thisRating % 100 == 0):\n",
        "                print(thisRating, \" of \", self.trainset.n_items)\n",
        "            for otherRating in range(thisRating+1, self.trainset.n_items):\n",
        "                thisMovieID = int(self.trainset.to_raw_iid(thisRating))\n",
        "                otherMovieID = int(self.trainset.to_raw_iid(otherRating))\n",
        "                featuresSimilarity = self.computeGenreSimilarity(thisMovieID, otherMovieID, features)\n",
        "                self.similarities[thisRating, otherRating] = featuresSimilarity\n",
        "                self.similarities[otherRating, thisRating] = self.similarities[thisRating, otherRating]\n",
        "                \n",
        "        print(\"...done.\")\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    def computeGenreSimilarity(self, movie1, movie2, genres):\n",
        "        genres1 = genres[movie1]\n",
        "        genres2 = genres[movie2]\n",
        "        sumxx, sumxy, sumyy = 0, 0, 0\n",
        "        for i in range(len(genres1)):\n",
        "            x = genres1[i]\n",
        "            y = genres2[i]\n",
        "            sumxx += x * x\n",
        "            sumyy += y * y\n",
        "            sumxy += x * y\n",
        "        \n",
        "        return sumxy/math.sqrt(sumxx*sumyy)\n",
        "    \n",
        "    def computeYearSimilarity(self, movie1, movie2, years):\n",
        "        diff = abs(years[movie1] - years[movie2])\n",
        "        sim = math.exp(-diff / 10.0)\n",
        "        return sim\n",
        "    \n",
        "    def computeMiseEnSceneSimilarity(self, movie1, movie2, mes):\n",
        "        mes1 = mes[movie1]\n",
        "        mes2 = mes[movie2]\n",
        "        if (mes1 and mes2):\n",
        "            shotLengthDiff = math.fabs(mes1[0] - mes2[0])\n",
        "            colorVarianceDiff = math.fabs(mes1[1] - mes2[1])\n",
        "            motionDiff = math.fabs(mes1[3] - mes2[3])\n",
        "            lightingDiff = math.fabs(mes1[5] - mes2[5])\n",
        "            numShotsDiff = math.fabs(mes1[6] - mes2[6])\n",
        "            return shotLengthDiff * colorVarianceDiff * motionDiff * lightingDiff * numShotsDiff\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unkown.')\n",
        "        \n",
        "        # Build up similarity scores between this item and everything the user rated\n",
        "        neighbors = []\n",
        "        for rating in self.trainset.ur[u]:\n",
        "            genreSimilarity = self.similarities[i,rating[0]]\n",
        "            neighbors.append( (genreSimilarity, rating[1]) )\n",
        "        \n",
        "        # Extract the top-K most-similar ratings\n",
        "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n",
        "        \n",
        "        # Compute average sim score of K neighbors weighted by user ratings\n",
        "        simTotal = weightedSum = 0\n",
        "        for (simScore, rating) in k_neighbors:\n",
        "            if (simScore > 0):\n",
        "                simTotal += simScore\n",
        "                weightedSum += simScore * rating\n",
        "            \n",
        "        if (simTotal == 0):\n",
        "            raise PredictionImpossible('No neighbors')\n",
        "\n",
        "        predictedRating = weightedSum / simTotal\n",
        "\n",
        "        return predictedRating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ym_cAP9H9z"
      },
      "source": [
        "## Avaliador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2RiTf2znthT"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "class EvaluationData:\n",
        "    \n",
        "    def __init__(self, data, popularityRankings):\n",
        "        \n",
        "        self.rankings = popularityRankings\n",
        "        \n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "        \n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=TEST_SIZE, random_state=1)\n",
        "        \n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "            \n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "        \n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "            \n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "    \n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "    \n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "    \n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "    \n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "    \n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "    \n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "    \n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "    \n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRa8c9iTnthT"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=4.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "\n",
        "        for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(movieID), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutMovieID = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == int(movieID)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOutMovieID) == movieID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                movie1 = pair[0][0]\n",
        "                movie2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        S = total / n\n",
        "        return (1-S)\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                movieID = rating[0]\n",
        "                rank = rankings[movieID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfDzJBP5nthU"
      },
      "outputs": [],
      "source": [
        "class EvaluatedAlgorithm:\n",
        "    \n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "        \n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "        \n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())        \n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a movie the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)   \n",
        "            # See how often we recommended a movie the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "        \n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted, \n",
        "                                                                   evaluationData.GetFullTrainSet().n_users, \n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "            \n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted, \n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "        \n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "    \n",
        "        return metrics\n",
        "    \n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "    \n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwdtxR1CnthU"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    \n",
        "    algorithms = []\n",
        "    \n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "        \n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "        \n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if (doTopN):\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "                \n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "        \n",
        "    \n",
        "    def SampleTopNRecs(self, ml, testSubject=USER_KEY_TEST, k=K_TOP):\n",
        "        \n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "            \n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "            \n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "        \n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "            \n",
        "            recommendations = []\n",
        "            \n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "                intMovieID = int(movieID)\n",
        "                recommendations.append((intMovieID, estimatedRating))\n",
        "            \n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ratings[0], ratings[1])\n",
        "                # print(ml.getMovieName(ratings[0]), ratings[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaqHNb31rgXK"
      },
      "source": [
        "## Rotina principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN9iL0OinthU",
        "outputId": "44c70457-131d-4a1a-bcc6-0a27b2c26352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ratings...\n",
            "\n",
            "Computing popularity ranks so we can measure novelty later...\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating  ContentKNN ...\n",
            "Evaluating accuracy...\n",
            "Computing content-based similarity matrix...\n",
            "0  of  3397\n",
            "100  of  3397\n",
            "200  of  3397\n",
            "300  of  3397\n",
            "400  of  3397\n",
            "500  of  3397\n",
            "600  of  3397\n",
            "700  of  3397\n",
            "800  of  3397\n",
            "900  of  3397\n",
            "1000  of  3397\n",
            "1100  of  3397\n",
            "1200  of  3397\n",
            "1300  of  3397\n",
            "1400  of  3397\n",
            "1500  of  3397\n",
            "1600  of  3397\n",
            "1700  of  3397\n",
            "1800  of  3397\n",
            "1900  of  3397\n",
            "2000  of  3397\n",
            "2100  of  3397\n",
            "2200  of  3397\n",
            "2300  of  3397\n",
            "2400  of  3397\n",
            "2500  of  3397\n",
            "2600  of  3397\n",
            "2700  of  3397\n",
            "2800  of  3397\n",
            "2900  of  3397\n",
            "3000  of  3397\n",
            "3100  of  3397\n",
            "3200  of  3397\n",
            "3300  of  3397\n",
            "...done.\n",
            "Evaluating top-N with leave-one-out...\n",
            "Computing content-based similarity matrix...\n",
            "0  of  3397\n",
            "100  of  3397\n",
            "200  of  3397\n",
            "300  of  3397\n",
            "400  of  3397\n",
            "500  of  3397\n",
            "600  of  3397\n",
            "700  of  3397\n",
            "800  of  3397\n",
            "900  of  3397\n",
            "1000  of  3397\n",
            "1100  of  3397\n",
            "1200  of  3397\n",
            "1300  of  3397\n",
            "1400  of  3397\n",
            "1500  of  3397\n",
            "1600  of  3397\n",
            "1700  of  3397\n",
            "1800  of  3397\n",
            "1900  of  3397\n",
            "2000  of  3397\n",
            "2100  of  3397\n",
            "2200  of  3397\n",
            "2300  of  3397\n",
            "2400  of  3397\n",
            "2500  of  3397\n",
            "2600  of  3397\n",
            "2700  of  3397\n",
            "2800  of  3397\n",
            "2900  of  3397\n",
            "3000  of  3397\n",
            "3100  of  3397\n",
            "3200  of  3397\n",
            "3300  of  3397\n",
            "...done.\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Computing content-based similarity matrix...\n",
            "0  of  3397\n",
            "100  of  3397\n",
            "200  of  3397\n",
            "300  of  3397\n",
            "400  of  3397\n",
            "500  of  3397\n",
            "600  of  3397\n",
            "700  of  3397\n",
            "800  of  3397\n",
            "900  of  3397\n",
            "1000  of  3397\n",
            "1100  of  3397\n",
            "1200  of  3397\n",
            "1300  of  3397\n",
            "1400  of  3397\n",
            "1500  of  3397\n",
            "1600  of  3397\n",
            "1700  of  3397\n",
            "1800  of  3397\n",
            "1900  of  3397\n",
            "2000  of  3397\n",
            "2100  of  3397\n",
            "2200  of  3397\n",
            "2300  of  3397\n",
            "2400  of  3397\n",
            "2500  of  3397\n",
            "2600  of  3397\n",
            "2700  of  3397\n",
            "2800  of  3397\n",
            "2900  of  3397\n",
            "3000  of  3397\n",
            "3100  of  3397\n",
            "3200  of  3397\n",
            "3300  of  3397\n",
            "...done.\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Random ...\n",
            "Evaluating accuracy...\n",
            "Evaluating top-N with leave-one-out...\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "\n",
            "\n",
            "Algorithm  RMSE       MAE        HR         cHR        ARHR       Coverage   Diversity  Novelty   \n",
            "ContentKNN 0.4278     0.2292     0.0050     0.0050     0.0016     0.9382     0.0110     1801.0132 \n",
            "Random     1.5868     1.2789     0.0074     0.0074     0.0024     1.0000     0.0690     1701.9739 \n",
            "\n",
            "Legend:\n",
            "\n",
            "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
            "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
            "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
            "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
            "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
            "Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
            "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
            "           for a given user. Higher means more diverse.\n",
            "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n",
            "\n",
            "Using recommender  ContentKNN\n",
            "\n",
            "Building recommendation model...\n",
            "Computing content-based similarity matrix...\n",
            "0  of  3397\n",
            "100  of  3397\n",
            "200  of  3397\n",
            "300  of  3397\n",
            "400  of  3397\n",
            "500  of  3397\n",
            "600  of  3397\n",
            "700  of  3397\n",
            "800  of  3397\n",
            "900  of  3397\n",
            "1000  of  3397\n",
            "1100  of  3397\n",
            "1200  of  3397\n",
            "1300  of  3397\n",
            "1400  of  3397\n",
            "1500  of  3397\n",
            "1600  of  3397\n",
            "1700  of  3397\n",
            "1800  of  3397\n",
            "1900  of  3397\n",
            "2000  of  3397\n",
            "2100  of  3397\n",
            "2200  of  3397\n",
            "2300  of  3397\n",
            "2400  of  3397\n",
            "2500  of  3397\n",
            "2600  of  3397\n",
            "2700  of  3397\n",
            "2800  of  3397\n",
            "2900  of  3397\n",
            "3000  of  3397\n",
            "3100  of  3397\n",
            "3200  of  3397\n",
            "3300  of  3397\n",
            "...done.\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "41340698 4.5506155004323725\n",
            "516238309 4.502799525747369\n",
            "40442027 4.495241632788425\n",
            "29818763 4.483687431282645\n",
            "62916113 4.4817793711248415\n",
            "52305371 4.477872702712288\n",
            "455992103 4.470193955849801\n",
            "478301469 4.4488606594958\n",
            "38046837 4.444424651893927\n",
            "80105412 4.427335083863202\n",
            "\n",
            "Using recommender  Random\n",
            "\n",
            "Building recommendation model...\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "65648397 5\n",
            "76376912 5\n",
            "67971380 5\n",
            "66731055 5\n",
            "117007988 5\n",
            "70815577 5\n",
            "527221686 5\n",
            "167810506 5\n",
            "117503216 5\n",
            "415520482 5\n"
          ]
        }
      ],
      "source": [
        "from surprise import NormalPredictor\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def LoadMovieLensData():\n",
        "    obj_dataset = CanaisTwitch()\n",
        "    print(\"Loading ratings...\")\n",
        "    data = obj_dataset.loadData()\n",
        "    print(\"\\nComputing popularity ranks so we can measure novelty later...\")\n",
        "    rankings = obj_dataset.getPopularityRanks()\n",
        "    return (obj_dataset, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(ml, evaluationData, rankings) = LoadMovieLensData()\n",
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings) \n",
        "\n",
        "contentKNN = ContentKNNAlgorithm()\n",
        "evaluator.AddAlgorithm(contentKNN, \"ContentKNN\")\n",
        "\n",
        "# Just make random recommendations\n",
        "Random = NormalPredictor()\n",
        "evaluator.AddAlgorithm(Random, \"Random\")\n",
        "\n",
        "evaluator.Evaluate(True)\n",
        "\n",
        "evaluator.SampleTopNRecs(ml)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TWsi5y5PN1LZ",
        "FpdQgtDH9BEo"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}